{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-03-21</th>\n",
       "      <td>102.1213</td>\n",
       "      <td>103.7794</td>\n",
       "      <td>101.3598</td>\n",
       "      <td>102.1020</td>\n",
       "      <td>35502678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-22</th>\n",
       "      <td>101.4657</td>\n",
       "      <td>103.4324</td>\n",
       "      <td>101.4272</td>\n",
       "      <td>102.8829</td>\n",
       "      <td>32444375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-23</th>\n",
       "      <td>102.6515</td>\n",
       "      <td>103.2203</td>\n",
       "      <td>102.0924</td>\n",
       "      <td>102.3141</td>\n",
       "      <td>25703495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-24</th>\n",
       "      <td>101.6778</td>\n",
       "      <td>102.4298</td>\n",
       "      <td>101.1187</td>\n",
       "      <td>101.8706</td>\n",
       "      <td>26132955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-28</th>\n",
       "      <td>102.1888</td>\n",
       "      <td>102.3719</td>\n",
       "      <td>101.2826</td>\n",
       "      <td>101.4079</td>\n",
       "      <td>19411372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close    volume\n",
       "date                                                        \n",
       "2016-03-21  102.1213  103.7794  101.3598  102.1020  35502678\n",
       "2016-03-22  101.4657  103.4324  101.4272  102.8829  32444375\n",
       "2016-03-23  102.6515  103.2203  102.0924  102.3141  25703495\n",
       "2016-03-24  101.6778  102.4298  101.1187  101.8706  26132955\n",
       "2016-03-28  102.1888  102.3719  101.2826  101.4079  19411372"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import datetime\n",
    "from pandas_datareader import data\n",
    "\n",
    "stock = 'AAPL'\n",
    "start = datetime(2016,3,19)\n",
    "end = datetime(2018,3,19)\n",
    "\n",
    "df = data.DataReader(stock, 'iex', start, end)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python_Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>macd</th>\n",
       "      <th>rsi_12</th>\n",
       "      <th>volume_delta</th>\n",
       "      <th>MA 20</th>\n",
       "      <th>MA 50</th>\n",
       "      <th>Daily Change</th>\n",
       "      <th>Fluctuation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>113.8474</td>\n",
       "      <td>28781865</td>\n",
       "      <td>1.277287</td>\n",
       "      <td>52.196938</td>\n",
       "      <td>-1804400.0</td>\n",
       "      <td>112.612895</td>\n",
       "      <td>110.465704</td>\n",
       "      <td>0.3430</td>\n",
       "      <td>1.368094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>113.7200</td>\n",
       "      <td>21118116</td>\n",
       "      <td>1.188925</td>\n",
       "      <td>49.671961</td>\n",
       "      <td>-7663749.0</td>\n",
       "      <td>112.951545</td>\n",
       "      <td>110.466010</td>\n",
       "      <td>0.1666</td>\n",
       "      <td>0.656558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>114.2983</td>\n",
       "      <td>22193587</td>\n",
       "      <td>1.152279</td>\n",
       "      <td>60.167694</td>\n",
       "      <td>1075471.0</td>\n",
       "      <td>113.277945</td>\n",
       "      <td>110.457402</td>\n",
       "      <td>0.6763</td>\n",
       "      <td>0.910283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <td>115.5725</td>\n",
       "      <td>31751900</td>\n",
       "      <td>1.212082</td>\n",
       "      <td>74.324962</td>\n",
       "      <td>9558313.0</td>\n",
       "      <td>113.615125</td>\n",
       "      <td>110.462576</td>\n",
       "      <td>1.1076</td>\n",
       "      <td>1.451020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-09</th>\n",
       "      <td>116.6311</td>\n",
       "      <td>33561948</td>\n",
       "      <td>1.329570</td>\n",
       "      <td>81.042392</td>\n",
       "      <td>1810048.0</td>\n",
       "      <td>113.951815</td>\n",
       "      <td>110.540802</td>\n",
       "      <td>1.0193</td>\n",
       "      <td>1.263300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               close    volume      macd     rsi_12  volume_delta       MA 20  \\\n",
       "date                                                                            \n",
       "2017-01-03  113.8474  28781865  1.277287  52.196938    -1804400.0  112.612895   \n",
       "2017-01-04  113.7200  21118116  1.188925  49.671961    -7663749.0  112.951545   \n",
       "2017-01-05  114.2983  22193587  1.152279  60.167694     1075471.0  113.277945   \n",
       "2017-01-06  115.5725  31751900  1.212082  74.324962     9558313.0  113.615125   \n",
       "2017-01-09  116.6311  33561948  1.329570  81.042392     1810048.0  113.951815   \n",
       "\n",
       "                 MA 50  Daily Change  Fluctuation  \n",
       "date                                               \n",
       "2017-01-03  110.465704        0.3430     1.368094  \n",
       "2017-01-04  110.466010        0.1666     0.656558  \n",
       "2017-01-05  110.457402        0.6763     0.910283  \n",
       "2017-01-06  110.462576        1.1076     1.451020  \n",
       "2017-01-09  110.540802        1.0193     1.263300  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stockstats import StockDataFrame\n",
    "df = StockDataFrame.retype(df)\n",
    "y = df['close']\n",
    "\n",
    "#generate features with stockstats package\n",
    "df['macd'] = df.get('macd')\n",
    "df['rsi_12'] = df.get('rsi_6')\n",
    "df['volume_delta'] = df.get('volume_delta')\n",
    "df['MA 20'] = y.rolling(20).mean()\n",
    "df['MA 50'] = y.rolling(50).mean()\n",
    "df['Daily Change'] = df['close']-df['open']\n",
    "df['Fluctuation'] = ((df['high']-df['low'])/df['low'])*100\n",
    "df.tail(10)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "X = df.drop(['open', 'high', 'low', 'macdh', 'macds', 'close_-1_s', 'close_-1_d', 'rs_6', 'rsi_6', 'close_12_ema', 'close_26_ema'], axis=1)\n",
    "X_shape = X.shape[0]\n",
    "X1 = X['20160103':'20180319']\n",
    "y = X1['close']\n",
    "y_max = np.max(np.array(y))\n",
    "y_min = np.min(np.array(y))\n",
    "plt.plot(np.array(y))\n",
    "X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_df(df):\n",
    "    normalize_data = MinMaxScaler()\n",
    "    for i in range(0,df.shape[1]):\n",
    "        df.iloc[:,i] = normalize_data.fit_transform(df.iloc[:,i].values.reshape(-1,1))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python_Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "D:\\python_Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\python_Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexing.py:621: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>macd</th>\n",
       "      <th>rsi_12</th>\n",
       "      <th>volume_delta</th>\n",
       "      <th>MA 20</th>\n",
       "      <th>MA 50</th>\n",
       "      <th>Daily Change</th>\n",
       "      <th>Fluctuation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-06</th>\n",
       "      <td>0.099653</td>\n",
       "      <td>0.725616</td>\n",
       "      <td>0.619796</td>\n",
       "      <td>0.522207</td>\n",
       "      <td>0.897092</td>\n",
       "      <td>0.976736</td>\n",
       "      <td>0.342433</td>\n",
       "      <td>0.130144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-07</th>\n",
       "      <td>0.180452</td>\n",
       "      <td>0.709808</td>\n",
       "      <td>0.489263</td>\n",
       "      <td>0.611019</td>\n",
       "      <td>0.906962</td>\n",
       "      <td>0.976972</td>\n",
       "      <td>0.435405</td>\n",
       "      <td>0.079564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-08</th>\n",
       "      <td>0.099506</td>\n",
       "      <td>0.711144</td>\n",
       "      <td>0.604323</td>\n",
       "      <td>0.498695</td>\n",
       "      <td>0.921030</td>\n",
       "      <td>0.977823</td>\n",
       "      <td>0.531174</td>\n",
       "      <td>0.124576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-09</th>\n",
       "      <td>0.185369</td>\n",
       "      <td>0.735690</td>\n",
       "      <td>0.730379</td>\n",
       "      <td>0.614536</td>\n",
       "      <td>0.940876</td>\n",
       "      <td>0.981080</td>\n",
       "      <td>0.570320</td>\n",
       "      <td>0.175727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-12</th>\n",
       "      <td>0.185593</td>\n",
       "      <td>0.766874</td>\n",
       "      <td>0.782677</td>\n",
       "      <td>0.555064</td>\n",
       "      <td>0.960605</td>\n",
       "      <td>0.984889</td>\n",
       "      <td>0.529077</td>\n",
       "      <td>0.131173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>0.180351</td>\n",
       "      <td>0.772664</td>\n",
       "      <td>0.639817</td>\n",
       "      <td>0.551268</td>\n",
       "      <td>0.974059</td>\n",
       "      <td>0.987979</td>\n",
       "      <td>0.245965</td>\n",
       "      <td>0.329905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-14</th>\n",
       "      <td>0.156614</td>\n",
       "      <td>0.760396</td>\n",
       "      <td>0.532662</td>\n",
       "      <td>0.538425</td>\n",
       "      <td>0.985049</td>\n",
       "      <td>0.991169</td>\n",
       "      <td>0.297694</td>\n",
       "      <td>0.184713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-15</th>\n",
       "      <td>0.088988</td>\n",
       "      <td>0.749200</td>\n",
       "      <td>0.544918</td>\n",
       "      <td>0.507945</td>\n",
       "      <td>0.993842</td>\n",
       "      <td>0.993455</td>\n",
       "      <td>0.439599</td>\n",
       "      <td>0.132683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16</th>\n",
       "      <td>0.259069</td>\n",
       "      <td>0.731690</td>\n",
       "      <td>0.495103</td>\n",
       "      <td>0.673021</td>\n",
       "      <td>0.997763</td>\n",
       "      <td>0.995547</td>\n",
       "      <td>0.385074</td>\n",
       "      <td>0.068982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-19</th>\n",
       "      <td>0.198248</td>\n",
       "      <td>0.691325</td>\n",
       "      <td>0.325568</td>\n",
       "      <td>0.512671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996506</td>\n",
       "      <td>0.287907</td>\n",
       "      <td>0.298782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              volume      macd    rsi_12  volume_delta     MA 20     MA 50  \\\n",
       "date                                                                         \n",
       "2018-03-06  0.099653  0.725616  0.619796      0.522207  0.897092  0.976736   \n",
       "2018-03-07  0.180452  0.709808  0.489263      0.611019  0.906962  0.976972   \n",
       "2018-03-08  0.099506  0.711144  0.604323      0.498695  0.921030  0.977823   \n",
       "2018-03-09  0.185369  0.735690  0.730379      0.614536  0.940876  0.981080   \n",
       "2018-03-12  0.185593  0.766874  0.782677      0.555064  0.960605  0.984889   \n",
       "2018-03-13  0.180351  0.772664  0.639817      0.551268  0.974059  0.987979   \n",
       "2018-03-14  0.156614  0.760396  0.532662      0.538425  0.985049  0.991169   \n",
       "2018-03-15  0.088988  0.749200  0.544918      0.507945  0.993842  0.993455   \n",
       "2018-03-16  0.259069  0.731690  0.495103      0.673021  0.997763  0.995547   \n",
       "2018-03-19  0.198248  0.691325  0.325568      0.512671  1.000000  0.996506   \n",
       "\n",
       "            Daily Change  Fluctuation  \n",
       "date                                   \n",
       "2018-03-06      0.342433     0.130144  \n",
       "2018-03-07      0.435405     0.079564  \n",
       "2018-03-08      0.531174     0.124576  \n",
       "2018-03-09      0.570320     0.175727  \n",
       "2018-03-12      0.529077     0.131173  \n",
       "2018-03-13      0.245965     0.329905  \n",
       "2018-03-14      0.297694     0.184713  \n",
       "2018-03-15      0.439599     0.132683  \n",
       "2018-03-16      0.385074     0.068982  \n",
       "2018-03-19      0.287907     0.298782  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = normalize_df(X1)\n",
    "y_normal = X2['close']\n",
    "\n",
    "\n",
    "X3 = X2.drop(['close'], axis = 1)\n",
    "X3.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 10 #prediction time-lag window\n",
    "\n",
    "def load_data(stock,y,window):\n",
    "    raw_data = stock.as_matrix()\n",
    "    length = raw_data.shape[0]\n",
    "    indicators = raw_data.shape[1]    \n",
    "    prices = y.as_matrix()\n",
    "    data = []\n",
    "\n",
    "    for index in range(len(raw_data)-(window)+1):\n",
    "        data.append(raw_data[index:index+window])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    valid_size = int(np.round(1/100*data.shape[0]))\n",
    "    test_size = int(1)\n",
    "    training_size = data.shape[0] - (valid_size + test_size)\n",
    "    \n",
    "    X_train = data[:training_size+valid_size,:-1]\n",
    "    y_train = prices[window-1:training_size+window+valid_size-1]\n",
    "    \n",
    "    #no need\n",
    "    #X_valid = data[training_size:training_size+valid_size,:-1]\n",
    "    #y_valid = prices[training_size+window-1:training_size+valid_size+window-1]\n",
    "    \n",
    "    X_valid = data[training_size+valid_size:,:-1]\n",
    "    y_valid = prices[-1]\n",
    "    \n",
    "    raw_data = raw_data.reshape(1,length,indicators)\n",
    "    \n",
    "    X_test = np.zeros((1,window,8))\n",
    "    for i in range(0,window-1):\n",
    "        X_test[0,window-1-i,:] = raw_data[0,length-i-1,:]\n",
    "    X_test = X_test[:,1:,:]\n",
    "    y_test = 0\n",
    "    \n",
    "    return [X_train, y_train, X_valid, y_valid, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 8)\n",
      "[[[ 0.09965288  0.72561585  0.61979555  0.52220658  0.89709181  0.97673627\n",
      "    0.34243253  0.13014397]\n",
      "  [ 0.18045206  0.70980775  0.48926335  0.61101938  0.90696178  0.97697184\n",
      "    0.43540506  0.07956419]\n",
      "  [ 0.09950588  0.71114431  0.60432252  0.49869512  0.92102991  0.97782318\n",
      "    0.53117376  0.12457605]\n",
      "  [ 0.18536945  0.73568981  0.73037875  0.61453635  0.94087578  0.98108023\n",
      "    0.57032009  0.17572749]\n",
      "  [ 0.18559321  0.7668744   0.78267664  0.55506368  0.96060464  0.98488859\n",
      "    0.52907664  0.13117311]\n",
      "  [ 0.18035066  0.7726635   0.63981667  0.55126759  0.97405861  0.98797867\n",
      "    0.24596478  0.32990465]\n",
      "  [ 0.15661432  0.76039623  0.53266204  0.53842456  0.98504941  0.99116946\n",
      "    0.29769386  0.18471287]\n",
      "  [ 0.08898806  0.74919954  0.544918    0.50794515  0.99384204  0.99345506\n",
      "    0.43959931  0.1326828 ]\n",
      "  [ 0.2590694   0.73169042  0.49510275  0.67302148  0.99776287  0.9955472\n",
      "    0.38507406  0.06898228]\n",
      "  [ 0.19824849  0.69132485  0.32556818  0.51267114  1.          0.99650558\n",
      "    0.28790728  0.29878211]]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_data(X3,y_normal,window+1)\n",
    "print(X_test.shape)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.02105879783630371\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "\n",
    "def build_model(layers,neurons,d):\n",
    "    #d = 0.3\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    \n",
    "        \n",
    "    start = time.time()\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "model = build_model([X3.shape[1],window,1],[256,256,32,1],0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 263 samples, validate on 30 samples\n",
      "Epoch 1/90\n",
      "263/263 [==============================] - 4s 13ms/step - loss: 0.3666 - acc: 0.0000e+00 - val_loss: 0.7180 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "263/263 [==============================] - 0s 141us/step - loss: 0.3543 - acc: 0.0000e+00 - val_loss: 0.6921 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "263/263 [==============================] - 0s 133us/step - loss: 0.3384 - acc: 0.0000e+00 - val_loss: 0.6524 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "263/263 [==============================] - 0s 133us/step - loss: 0.3147 - acc: 0.0000e+00 - val_loss: 0.5880 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "263/263 [==============================] - 0s 135us/step - loss: 0.2761 - acc: 0.0000e+00 - val_loss: 0.4858 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "263/263 [==============================] - 0s 135us/step - loss: 0.2168 - acc: 0.0000e+00 - val_loss: 0.3363 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "263/263 [==============================] - 0s 133us/step - loss: 0.1342 - acc: 0.0000e+00 - val_loss: 0.1641 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0539 - acc: 0.0000e+00 - val_loss: 0.0388 - val_acc: 0.0333\n",
      "Epoch 9/90\n",
      "263/263 [==============================] - 0s 139us/step - loss: 0.0447 - acc: 0.0000e+00 - val_loss: 0.0132 - val_acc: 0.0333\n",
      "Epoch 10/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0886 - acc: 0.0000e+00 - val_loss: 0.0142 - val_acc: 0.0333\n",
      "Epoch 11/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0900 - acc: 0.0000e+00 - val_loss: 0.0237 - val_acc: 0.0333\n",
      "Epoch 12/90\n",
      "263/263 [==============================] - 0s 141us/step - loss: 0.0685 - acc: 0.0000e+00 - val_loss: 0.0457 - val_acc: 0.0333\n",
      "Epoch 13/90\n",
      "263/263 [==============================] - 0s 133us/step - loss: 0.0512 - acc: 0.0000e+00 - val_loss: 0.0770 - val_acc: 0.0333\n",
      "Epoch 14/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0432 - acc: 0.0000e+00 - val_loss: 0.1103 - val_acc: 0.0333\n",
      "Epoch 15/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0444 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0333\n",
      "Epoch 16/90\n",
      "263/263 [==============================] - 0s 145us/step - loss: 0.0490 - acc: 0.0000e+00 - val_loss: 0.1590 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "263/263 [==============================] - 0s 128us/step - loss: 0.0552 - acc: 0.0000e+00 - val_loss: 0.1698 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0574 - acc: 0.0000e+00 - val_loss: 0.1720 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "263/263 [==============================] - 0s 147us/step - loss: 0.0595 - acc: 0.0000e+00 - val_loss: 0.1668 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0572 - acc: 0.0000e+00 - val_loss: 0.1558 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0545 - acc: 0.0000e+00 - val_loss: 0.1406 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0502 - acc: 0.0000e+00 - val_loss: 0.1228 - val_acc: 0.0333\n",
      "Epoch 23/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0451 - acc: 0.0000e+00 - val_loss: 0.1042 - val_acc: 0.0333\n",
      "Epoch 24/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0435 - acc: 0.0000e+00 - val_loss: 0.0866 - val_acc: 0.0333\n",
      "Epoch 25/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0420 - acc: 0.0000e+00 - val_loss: 0.0716 - val_acc: 0.0333\n",
      "Epoch 26/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0430 - acc: 0.0000e+00 - val_loss: 0.0602 - val_acc: 0.0333\n",
      "Epoch 27/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0443 - acc: 0.0000e+00 - val_loss: 0.0528 - val_acc: 0.0333\n",
      "Epoch 28/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0430 - acc: 0.0000e+00 - val_loss: 0.0494 - val_acc: 0.0333\n",
      "Epoch 29/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0450 - acc: 0.0000e+00 - val_loss: 0.0497 - val_acc: 0.0333\n",
      "Epoch 30/90\n",
      "263/263 [==============================] - 0s 137us/step - loss: 0.0452 - acc: 0.0000e+00 - val_loss: 0.0535 - val_acc: 0.0333\n",
      "Epoch 31/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0415 - acc: 0.0000e+00 - val_loss: 0.0602 - val_acc: 0.0333\n",
      "Epoch 32/90\n",
      "263/263 [==============================] - 0s 137us/step - loss: 0.0381 - acc: 0.0000e+00 - val_loss: 0.0683 - val_acc: 0.0333\n",
      "Epoch 33/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0355 - acc: 0.0000e+00 - val_loss: 0.0772 - val_acc: 0.0333\n",
      "Epoch 34/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0331 - acc: 0.0000e+00 - val_loss: 0.0854 - val_acc: 0.0333\n",
      "Epoch 35/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0332 - acc: 0.0000e+00 - val_loss: 0.0912 - val_acc: 0.0333\n",
      "Epoch 36/90\n",
      "263/263 [==============================] - 0s 145us/step - loss: 0.0334 - acc: 0.0000e+00 - val_loss: 0.0930 - val_acc: 0.0333\n",
      "Epoch 37/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0313 - acc: 0.0000e+00 - val_loss: 0.0895 - val_acc: 0.0333\n",
      "Epoch 38/90\n",
      "263/263 [==============================] - 0s 118us/step - loss: 0.0289 - acc: 0.0000e+00 - val_loss: 0.0800 - val_acc: 0.0333\n",
      "Epoch 39/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0265 - acc: 0.0000e+00 - val_loss: 0.0656 - val_acc: 0.0333\n",
      "Epoch 40/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0213 - acc: 0.0000e+00 - val_loss: 0.0486 - val_acc: 0.0333\n",
      "Epoch 41/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.0331 - val_acc: 0.0333\n",
      "Epoch 42/90\n",
      "263/263 [==============================] - 0s 133us/step - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.0226 - val_acc: 0.0333\n",
      "Epoch 43/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0134 - acc: 0.0000e+00 - val_loss: 0.0178 - val_acc: 0.0333\n",
      "Epoch 44/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0147 - acc: 0.0000e+00 - val_loss: 0.0169 - val_acc: 0.0333\n",
      "Epoch 45/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0174 - acc: 0.0000e+00 - val_loss: 0.0173 - val_acc: 0.0333\n",
      "Epoch 46/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0164 - acc: 0.0000e+00 - val_loss: 0.0183 - val_acc: 0.0333\n",
      "Epoch 47/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.0195 - val_acc: 0.0333\n",
      "Epoch 48/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0160 - acc: 0.0000e+00 - val_loss: 0.0202 - val_acc: 0.0333\n",
      "Epoch 49/90\n",
      "263/263 [==============================] - 0s 118us/step - loss: 0.0138 - acc: 0.0000e+00 - val_loss: 0.0196 - val_acc: 0.0333\n",
      "Epoch 50/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0183 - val_acc: 0.0333\n",
      "Epoch 51/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0090 - acc: 0.0000e+00 - val_loss: 0.0178 - val_acc: 0.0333\n",
      "Epoch 52/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0092 - acc: 0.0000e+00 - val_loss: 0.0186 - val_acc: 0.0333\n",
      "Epoch 53/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0088 - acc: 0.0000e+00 - val_loss: 0.0209 - val_acc: 0.0333\n",
      "Epoch 54/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0089 - acc: 0.0000e+00 - val_loss: 0.0240 - val_acc: 0.0333\n",
      "Epoch 55/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0255 - val_acc: 0.0333\n",
      "Epoch 56/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.0240 - val_acc: 0.0333\n",
      "Epoch 57/90\n",
      "263/263 [==============================] - 0s 118us/step - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.0203 - val_acc: 0.0333\n",
      "Epoch 58/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0173 - val_acc: 0.0333\n",
      "Epoch 59/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0163 - val_acc: 0.0333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/90\n",
      "263/263 [==============================] - 0s 137us/step - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.0162 - val_acc: 0.0333\n",
      "Epoch 61/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0168 - val_acc: 0.0333\n",
      "Epoch 62/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0172 - val_acc: 0.0333\n",
      "Epoch 63/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0168 - val_acc: 0.0333\n",
      "Epoch 64/90\n",
      "263/263 [==============================] - 0s 120us/step - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0179 - val_acc: 0.0333\n",
      "Epoch 65/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0188 - val_acc: 0.0333\n",
      "Epoch 66/90\n",
      "263/263 [==============================] - 0s 133us/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0174 - val_acc: 0.0333\n",
      "Epoch 67/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0168 - val_acc: 0.0333\n",
      "Epoch 68/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0165 - val_acc: 0.0333\n",
      "Epoch 69/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0164 - val_acc: 0.0333\n",
      "Epoch 70/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0161 - val_acc: 0.0333\n",
      "Epoch 71/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0156 - val_acc: 0.0333\n",
      "Epoch 72/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0168 - val_acc: 0.0333\n",
      "Epoch 73/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0159 - val_acc: 0.0333\n",
      "Epoch 74/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0148 - val_acc: 0.0333\n",
      "Epoch 75/90\n",
      "263/263 [==============================] - 0s 124us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0147 - val_acc: 0.0333\n",
      "Epoch 76/90\n",
      "263/263 [==============================] - 0s 120us/step - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0161 - val_acc: 0.0333\n",
      "Epoch 77/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0171 - val_acc: 0.0333\n",
      "Epoch 78/90\n",
      "263/263 [==============================] - 0s 128us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0158 - val_acc: 0.0333\n",
      "Epoch 79/90\n",
      "263/263 [==============================] - 0s 118us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0147 - val_acc: 0.0333\n",
      "Epoch 80/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0148 - val_acc: 0.0333\n",
      "Epoch 81/90\n",
      "263/263 [==============================] - 0s 120us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0152 - val_acc: 0.0333\n",
      "Epoch 82/90\n",
      "263/263 [==============================] - 0s 120us/step - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0157 - val_acc: 0.0333\n",
      "Epoch 83/90\n",
      "263/263 [==============================] - 0s 130us/step - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0148 - val_acc: 0.0333\n",
      "Epoch 84/90\n",
      "263/263 [==============================] - 0s 124us/step - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0141 - val_acc: 0.0333\n",
      "Epoch 85/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0135 - val_acc: 0.0333\n",
      "Epoch 86/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0136 - val_acc: 0.0333\n",
      "Epoch 87/90\n",
      "263/263 [==============================] - 0s 132us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0333\n",
      "Epoch 88/90\n",
      "263/263 [==============================] - 0s 133us/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0137 - val_acc: 0.0333\n",
      "Epoch 89/90\n",
      "263/263 [==============================] - 0s 122us/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0134 - val_acc: 0.0333\n",
      "Epoch 90/90\n",
      "263/263 [==============================] - 0s 126us/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0133 - val_acc: 0.0333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136d96dc7b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=90,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 176.29493713]]\n",
      "175.3\n",
      "[[ 0.99493408]]\n"
     ]
    }
   ],
   "source": [
    "y_validation = model.predict(X_valid)\n",
    "\n",
    "y_validation_scaled = y_validation*(y_max-y_min)+y_min\n",
    "y_valid_scaled = y[-1]*(y_max-y_min)+y_min\n",
    "\n",
    "RMSE = ((y_validation_scaled-y_valid_scaled)**(2))**(0.5)\n",
    "\n",
    "print(y_validation_scaled)\n",
    "print(y_valid_scaled)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 176.52059937]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict_scaled = y_predict*(y_max-y_min)+y_min\n",
    "print(y_predict_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.3\n",
      "Buy\n"
     ]
    }
   ],
   "source": [
    "#Decision based on prediction\n",
    "y_check = y[-1]*(y_max-y_min)+y_min\n",
    "\n",
    "print(y_check)\n",
    "if y_check < y_predict_scaled:\n",
    "    print('Buy')\n",
    "elif y_check > y_predict_scaled:\n",
    "    print('Sell if bought')\n",
    "else:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.053766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.043531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.057369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.057225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.054199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.057369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.058522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.056937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.044541\n",
       "1  0.053766\n",
       "2  0.046559\n",
       "3  0.043531\n",
       "4  0.057369\n",
       "5  0.057225\n",
       "6  0.054199\n",
       "7  0.057369\n",
       "8  0.058522\n",
       "9  0.056937"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict 5 days ahead of time\n",
    "\n",
    "#adjust stock price\n",
    "y_normal1 = y_normal[5:]\n",
    "y_normal1.shape\n",
    "y_normal1 = np.array(y_normal1)\n",
    "y_normal2 = pd.DataFrame(np.append(y_normal1,[0,0,0,0,0]))\n",
    "y_normal2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock,y,window):\n",
    "    raw_data = stock.as_matrix()\n",
    "    length = raw_data.shape[0]\n",
    "    indicators = raw_data.shape[1]    \n",
    "    prices = y.as_matrix()\n",
    "    data = []\n",
    "\n",
    "    for index in range(len(raw_data)-(window)+1):\n",
    "        data.append(raw_data[index:index+window])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    valid_size = int(np.round(1/100*data.shape[0]))\n",
    "    test_size = int(1)\n",
    "    training_size = data.shape[0] - (4 + test_size)\n",
    "    \n",
    "    X_train = data[:training_size,:-1]\n",
    "    y_train = prices[window-1:training_size+window-1]\n",
    "    \n",
    "    #no need\n",
    "    #X_valid = data[training_size:training_size+valid_size,:-1]\n",
    "    #y_valid = prices[training_size+window-1:training_size+valid_size+window-1]\n",
    "    \n",
    "    X_valid = data[training_size+valid_size:,:-1]\n",
    "    y_valid = prices[-1]\n",
    "    \n",
    "    raw_data = raw_data.reshape(1,length,indicators)\n",
    "    \n",
    "    X_test = np.zeros((1,window,8))\n",
    "    for i in range(0,window-1):\n",
    "        X_test[0,window-1-i,:] = raw_data[0,length-i-1,:]\n",
    "    X_test = X_test[:,1:,:]\n",
    "    y_test = 0\n",
    "    \n",
    "    return [X_train, y_train, X_valid, y_valid, X_test, y_test]\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_data(X3,y_normal2,window+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 260 samples, validate on 29 samples\n",
      "Epoch 1/100\n",
      "260/260 [==============================] - 0s 471us/step - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0209 - val_acc: 0.0345\n",
      "Epoch 2/100\n",
      "260/260 [==============================] - 0s 208us/step - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0195 - val_acc: 0.0345\n",
      "Epoch 3/100\n",
      "260/260 [==============================] - 0s 143us/step - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0197 - val_acc: 0.0345\n",
      "Epoch 4/100\n",
      "260/260 [==============================] - 0s 139us/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0221 - val_acc: 0.0345\n",
      "Epoch 5/100\n",
      "260/260 [==============================] - 0s 150us/step - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.0241 - val_acc: 0.0345\n",
      "Epoch 6/100\n",
      "260/260 [==============================] - 0s 141us/step - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0221 - val_acc: 0.0345\n",
      "Epoch 7/100\n",
      "260/260 [==============================] - 0s 139us/step - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0205 - val_acc: 0.0345\n",
      "Epoch 8/100\n",
      "260/260 [==============================] - 0s 135us/step - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0209 - val_acc: 0.0345\n",
      "Epoch 9/100\n",
      "260/260 [==============================] - 0s 139us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0222 - val_acc: 0.0345\n",
      "Epoch 10/100\n",
      "260/260 [==============================] - 0s 143us/step - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0226 - val_acc: 0.0345\n",
      "Epoch 11/100\n",
      "260/260 [==============================] - 0s 131us/step - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0222 - val_acc: 0.0345\n",
      "Epoch 12/100\n",
      "260/260 [==============================] - 0s 139us/step - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0200 - val_acc: 0.0345\n",
      "Epoch 13/100\n",
      "260/260 [==============================] - 0s 139us/step - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0186 - val_acc: 0.0345\n",
      "Epoch 14/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0067 - acc: 0.0000e+00 - val_loss: 0.0190 - val_acc: 0.0345\n",
      "Epoch 15/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0208 - val_acc: 0.0345\n",
      "Epoch 16/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0208 - val_acc: 0.0345\n",
      "Epoch 17/100\n",
      "260/260 [==============================] - 0s 131us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0183 - val_acc: 0.0345\n",
      "Epoch 18/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0168 - val_acc: 0.0345\n",
      "Epoch 19/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0170 - val_acc: 0.0345\n",
      "Epoch 20/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0187 - val_acc: 0.0345\n",
      "Epoch 21/100\n",
      "260/260 [==============================] - 0s 131us/step - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0187 - val_acc: 0.0345\n",
      "Epoch 22/100\n",
      "260/260 [==============================] - 0s 131us/step - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0173 - val_acc: 0.0345\n",
      "Epoch 23/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0167 - val_acc: 0.0345\n",
      "Epoch 24/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0174 - val_acc: 0.0345\n",
      "Epoch 25/100\n",
      "260/260 [==============================] - 0s 129us/step - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0185 - val_acc: 0.0345\n",
      "Epoch 26/100\n",
      "260/260 [==============================] - 0s 125us/step - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0182 - val_acc: 0.0345\n",
      "Epoch 27/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0167 - val_acc: 0.0345\n",
      "Epoch 28/100\n",
      "260/260 [==============================] - 0s 137us/step - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0161 - val_acc: 0.0345\n",
      "Epoch 29/100\n",
      "260/260 [==============================] - 0s 125us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0170 - val_acc: 0.0345\n",
      "Epoch 30/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0182 - val_acc: 0.0345\n",
      "Epoch 31/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0176 - val_acc: 0.0345\n",
      "Epoch 32/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0167 - val_acc: 0.0345\n",
      "Epoch 33/100\n",
      "260/260 [==============================] - 0s 139us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0167 - val_acc: 0.0345\n",
      "Epoch 34/100\n",
      "260/260 [==============================] - 0s 131us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0162 - val_acc: 0.0345\n",
      "Epoch 35/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0163 - val_acc: 0.0345\n",
      "Epoch 36/100\n",
      "260/260 [==============================] - 0s 131us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0160 - val_acc: 0.0345\n",
      "Epoch 37/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0164 - val_acc: 0.0345\n",
      "Epoch 38/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0164 - val_acc: 0.0345\n",
      "Epoch 39/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0161 - val_acc: 0.0345\n",
      "Epoch 40/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0158 - val_acc: 0.0345\n",
      "Epoch 41/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0157 - val_acc: 0.0345\n",
      "Epoch 42/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0155 - val_acc: 0.0345\n",
      "Epoch 43/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0154 - val_acc: 0.0345\n",
      "Epoch 44/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0154 - val_acc: 0.0345\n",
      "Epoch 45/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0155 - val_acc: 0.0345\n",
      "Epoch 46/100\n",
      "260/260 [==============================] - 0s 125us/step - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0156 - val_acc: 0.0345\n",
      "Epoch 47/100\n",
      "260/260 [==============================] - 0s 125us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0155 - val_acc: 0.0345\n",
      "Epoch 48/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0147 - val_acc: 0.0345\n",
      "Epoch 49/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0149 - val_acc: 0.0345\n",
      "Epoch 50/100\n",
      "260/260 [==============================] - 0s 125us/step - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0160 - val_acc: 0.0345\n",
      "Epoch 51/100\n",
      "260/260 [==============================] - 0s 131us/step - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0153 - val_acc: 0.0345\n",
      "Epoch 52/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0345\n",
      "Epoch 53/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0145 - val_acc: 0.0345\n",
      "Epoch 54/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0164 - val_acc: 0.0345\n",
      "Epoch 55/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0152 - val_acc: 0.0345\n",
      "Epoch 56/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0345\n",
      "Epoch 57/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0141 - val_acc: 0.0345\n",
      "Epoch 58/100\n",
      "260/260 [==============================] - 0s 125us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0149 - val_acc: 0.0345\n",
      "Epoch 59/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0147 - val_acc: 0.0345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0141 - val_acc: 0.0345\n",
      "Epoch 61/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0142 - val_acc: 0.0345\n",
      "Epoch 62/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0142 - val_acc: 0.0345\n",
      "Epoch 63/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0145 - val_acc: 0.0345\n",
      "Epoch 64/100\n",
      "260/260 [==============================] - 0s 125us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0137 - val_acc: 0.0345\n",
      "Epoch 65/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0139 - val_acc: 0.0345\n",
      "Epoch 66/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0141 - val_acc: 0.0345\n",
      "Epoch 67/100\n",
      "260/260 [==============================] - 0s 125us/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0143 - val_acc: 0.0345\n",
      "Epoch 68/100\n",
      "260/260 [==============================] - 0s 121us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0144 - val_acc: 0.0345\n",
      "Epoch 69/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0144 - val_acc: 0.0345\n",
      "Epoch 70/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0137 - val_acc: 0.0345\n",
      "Epoch 71/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0141 - val_acc: 0.0345\n",
      "Epoch 72/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0149 - val_acc: 0.0345\n",
      "Epoch 73/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0142 - val_acc: 0.0345\n",
      "Epoch 74/100\n",
      "260/260 [==============================] - 0s 118us/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0141 - val_acc: 0.0345\n",
      "Epoch 75/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0139 - val_acc: 0.0345\n",
      "Epoch 76/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0146 - val_acc: 0.0345\n",
      "Epoch 77/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0145 - val_acc: 0.0345\n",
      "Epoch 78/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0139 - val_acc: 0.0345\n",
      "Epoch 79/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0145 - val_acc: 0.0345\n",
      "Epoch 80/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0143 - val_acc: 0.0345\n",
      "Epoch 81/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0138 - val_acc: 0.0345\n",
      "Epoch 82/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0135 - val_acc: 0.0345\n",
      "Epoch 83/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0137 - val_acc: 0.0345\n",
      "Epoch 84/100\n",
      "260/260 [==============================] - 0s 131us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0345\n",
      "Epoch 85/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0134 - val_acc: 0.0345\n",
      "Epoch 86/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0128 - val_acc: 0.0345\n",
      "Epoch 87/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0136 - val_acc: 0.0345\n",
      "Epoch 88/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0146 - val_acc: 0.0345\n",
      "Epoch 89/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0127 - val_acc: 0.0345\n",
      "Epoch 90/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0345\n",
      "Epoch 91/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0135 - val_acc: 0.0345\n",
      "Epoch 92/100\n",
      "260/260 [==============================] - 0s 127us/step - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0132 - val_acc: 0.0345\n",
      "Epoch 93/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0131 - val_acc: 0.0345\n",
      "Epoch 94/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0345\n",
      "Epoch 95/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0136 - val_acc: 0.0345\n",
      "Epoch 96/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0126 - val_acc: 0.0345\n",
      "Epoch 97/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0139 - val_acc: 0.0345\n",
      "Epoch 98/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0140 - val_acc: 0.0345\n",
      "Epoch 99/100\n",
      "260/260 [==============================] - 0s 120us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0130 - val_acc: 0.0345\n",
      "Epoch 100/100\n",
      "260/260 [==============================] - 0s 123us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0131 - val_acc: 0.0345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13840254908>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,batch_size=512,epochs=100,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 173.82354736]]\n",
      "175.3\n",
      "Sell if bought\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict_scaled = y_predict*(y_max-y_min)+y_min\n",
    "print(y_predict_scaled)\n",
    "\n",
    "#Decision based on prediction\n",
    "y_check = y[-1]*(y_max-y_min)+y_min\n",
    "\n",
    "print(y_check)\n",
    "if y_check < y_predict_scaled:\n",
    "    print('Buy')\n",
    "elif y_check > y_predict_scaled:\n",
    "    print('Sell if bought')\n",
    "else:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
